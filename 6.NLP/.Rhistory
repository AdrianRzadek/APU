text <- "Wikipedia.txt"
# Load the data as a corpus
TextDoc <- Corpus(VectorSource(text))
# Cleaning up Text Data
toSpace <- content_transformer(function (x, pattern) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
TextDoc <- tm_map(TextDoc, toSpace, "ˆa“")
TextDoc <- tm_map(TextDoc, toSpace, ":")
TextDoc <- tm_map(TextDoc, toSpace, ";")
TextDoc <- tm_map(TextDoc, toSpace, ",")
TextDoc <- tm_map(TextDoc, toSpace, "ˆI\nTM")
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
TextDoc <- tm_map(TextDoc, removeNumbers)
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team"))
TextDoc <- tm_map(TextDoc, removePunctuation)
TextDoc <- tm_map(TextDoc, stripWhitespace)
TextDoc <- tm_map(TextDoc, stemDocument)
TextDoc <- tm_map(TextDoc, content_transformer(function(x) gsub(x, pattern = "mathemat", replacement = "math")))
TextDoc <- tm_map(TextDoc, content_transformer(function(x) gsub(x, pattern = " r ", replacement = " Rlanguage ")))
# Building the term document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq = dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
# Plot the most frequent words
barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word, col = "lightgreen",
main = "Top 20 most frequent words in the forms of knowledge\nassessment for the Big Data training courses",
ylab = "Word frequencies")
# Generate the Word Cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, scale = c(5, 0.5), min.freq = 1,
max.words = 100, random.order = FALSE, rot.per = 0.40, colors = brewer.pal(8, "Dark2"))
# Word Association
findAssocs(TextDoc_dtm, terms = c("program", "algorithm", "math", "statist"), corlimit = 0.5)
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 30), corlimit = 0.5)
# Sentiment Scores
syuzhet_vector <- get_sentiment(text, method = "syuzhet")
head(syuzhet_vector)
summary(syuzhet_vector)
bing_vector <- get_sentiment(text, method = "bing")
head(bing_vector)
summary(bing_vector)
afinn_vector <- get_sentiment(text, method = "afinn")
head(afinn_vector)
summary(afinn_vector)
rbind(
sign(head(syuzhet_vector)),
sign(head(bing_vector)),
sign(head(afinn_vector))
)
# Emotion Classification
d <- get_nrc_sentiment(as.vector(dtm_d$word))
head(d, 10)
td <- data.frame(t(d))
td_new <- data.frame(rowSums(td[1:56]))
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
library("tidytext")
library("igraph")
library("ggraph")
library(dplyr)
library(tidyr)
text <- "Wikipedia.txt"
# Load the data as a corpus
TextDoc <- Corpus(VectorSource(text))
# Cleaning up Text Data
toSpace <- content_transformer(function (x, pattern) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
TextDoc <- tm_map(TextDoc, toSpace, "ˆa“")
TextDoc <- tm_map(TextDoc, toSpace, ":")
TextDoc <- tm_map(TextDoc, toSpace, ";")
TextDoc <- tm_map(TextDoc, toSpace, ",")
TextDoc <- tm_map(TextDoc, toSpace, "ˆI\nTM")
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
TextDoc <- tm_map(TextDoc, removeNumbers)
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team"))
TextDoc <- tm_map(TextDoc, removePunctuation)
TextDoc <- tm_map(TextDoc, stripWhitespace)
TextDoc <- tm_map(TextDoc, stemDocument)
TextDoc <- tm_map(TextDoc, content_transformer(function(x) gsub(x, pattern = "mathemat", replacement = "math")))
TextDoc <- tm_map(TextDoc, content_transformer(function(x) gsub(x, pattern = " r ", replacement = " Rlanguage ")))
# Building the term document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq = dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
# Plot the most frequent words
barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word, col = "lightgreen",
main = "Top 20 most frequent words in the forms of knowledge\nassessment for the Big Data training courses",
ylab = "Word frequencies")
# Generate the Word Cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, scale = c(5, 0.5), min.freq = 1,
max.words = 100, random.order = FALSE, rot.per = 0.40, colors = brewer.pal(8, "Dark2"))
# Word Association
findAssocs(TextDoc_dtm, terms = c("program", "algorithm", "math", "statist"), corlimit = 0.5)
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 30), corlimit = 0.5)
# Sentiment Scores
syuzhet_vector <- get_sentiment(text, method = "syuzhet")
head(syuzhet_vector)
summary(syuzhet_vector)
bing_vector <- get_sentiment(text, method = "bing")
head(bing_vector)
summary(bing_vector)
afinn_vector <- get_sentiment(text, method = "afinn")
head(afinn_vector)
summary(afinn_vector)
rbind(
sign(head(syuzhet_vector)),
sign(head(bing_vector)),
sign(head(afinn_vector))
)
# Emotion Classification
d <- get_nrc_sentiment(as.vector(dtm_d$word))
head(d, 10)
td <- data.frame(t(d))
td_new <- data.frame(rowSums(td[1:56]))
print(text)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
library("tidytext")
library("igraph")
library("ggraph")
library(dplyr)
library(tidyr)
text <- readPlain(./Wikipedia.txt)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
library("tidytext")
library("igraph")
library("ggraph")
library(dplyr)
library(tidyr)
text <- readPlain('./Wikipedia.txt')
text <- readLines('./Wikipedia.txt')
print(text)
# Load the data as a corpus
TextDoc <- Corpus(VectorSource(text))
# Cleaning up Text Data
toSpace <- content_transformer(function (x, pattern) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
TextDoc <- tm_map(TextDoc, toSpace, "ˆa“")
TextDoc <- tm_map(TextDoc, toSpace, ":")
TextDoc <- tm_map(TextDoc, toSpace, ";")
TextDoc <- tm_map(TextDoc, toSpace, ",")
TextDoc <- tm_map(TextDoc, toSpace, "ˆI\nTM")
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
TextDoc <- tm_map(TextDoc, removeNumbers)
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team"))
TextDoc <- tm_map(TextDoc, removePunctuation)
TextDoc <- tm_map(TextDoc, stripWhitespace)
TextDoc <- tm_map(TextDoc, stemDocument)
TextDoc <- tm_map(TextDoc, content_transformer(function(x) gsub(x, pattern = "mathemat", replacement = "math")))
TextDoc <- tm_map(TextDoc, content_transformer(function(x) gsub(x, pattern = " r ", replacement = " Rlanguage ")))
# Building the term document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq = dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
# Plot the most frequent words
barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word, col = "lightgreen",
main = "Top 20 most frequent words in the forms of knowledge\nassessment for the Big Data training courses",
ylab = "Word frequencies")
# Generate the Word Cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, scale = c(5, 0.5), min.freq = 1,
max.words = 100, random.order = FALSE, rot.per = 0.40, colors = brewer.pal(8, "Dark2"))
# Word Association
findAssocs(TextDoc_dtm, terms = c("program", "algorithm", "math", "statist"), corlimit = 0.5)
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 30), corlimit = 0.5)
# Sentiment Scores
syuzhet_vector <- get_sentiment(text, method = "syuzhet")
head(syuzhet_vector)
summary(syuzhet_vector)
bing_vector <- get_sentiment(text, method = "bing")
head(bing_vector)
summary(bing_vector)
afinn_vector <- get_sentiment(text, method = "afinn")
head(afinn_vector)
summary(afinn_vector)
rbind(
sign(head(syuzhet_vector)),
sign(head(bing_vector)),
sign(head(afinn_vector))
)
# Emotion Classification
d <- get_nrc_sentiment(as.vector(dtm_d$word))
head(d, 10)
td <- data.frame(t(d))
td_new <- data.frame(rowSums(td[1:56]))
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2 <- td_new[1:8,]
# Plot One - count of words associated with each sentiment
quickplot(sentiment, data = td_new2, weight = count, geom = "bar", fill = sentiment, ylab = "count") + ggtitle("Survey sentiments")
# Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
sort(colSums(prop.table(d[, 1:8]))),
horiz = TRUE,
cex.names = 0.7,
las = 1,
main = "Emotions in Text", xlab = "Percentage"
)
# Konstruowanie grafów powiązań
fileName <- 'MiarkaZaMiarke/measure_for_measure.txt'
text <- readChar(fileName, file.info(fileName)$size)
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
library("tidytext")
library("igraph")
library("ggraph")
library(dplyr)
library(tidyr)
text <- readLines('./Wikipedia.txt')
print(text)
# Load the data as a corpus
TextDoc <- Corpus(VectorSource(text))
# Cleaning up Text Data
toSpace <- content_transformer(function (x, pattern) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
TextDoc <- tm_map(TextDoc, toSpace, "ˆa“")
TextDoc <- tm_map(TextDoc, toSpace, ":")
TextDoc <- tm_map(TextDoc, toSpace, ";")
TextDoc <- tm_map(TextDoc, toSpace, ",")
TextDoc <- tm_map(TextDoc, toSpace, "ˆI\nTM")
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
TextDoc <- tm_map(TextDoc, removeNumbers)
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team"))
TextDoc <- tm_map(TextDoc, removePunctuation)
TextDoc <- tm_map(TextDoc, stripWhitespace)
TextDoc <- tm_map(TextDoc, stemDocument)
TextDoc <- tm_map(TextDoc, content_transformer(function(x) gsub(x, pattern = "mathemat", replacement = "math")))
TextDoc <- tm_map(TextDoc, content_transformer(function(x) gsub(x, pattern = " r ", replacement = " Rlanguage ")))
# Building the term document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq = dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
# Plot the most frequent words
barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word, col = "lightgreen",
main = "Top 20 most frequent words in the forms of knowledge\nassessment for the Big Data training courses",
ylab = "Word frequencies")
# Generate the Word Cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, scale = c(5, 0.5), min.freq = 1,
max.words = 100, random.order = FALSE, rot.per = 0.40, colors = brewer.pal(8, "Dark2"))
# Word Association
findAssocs(TextDoc_dtm, terms = c("program", "algorithm", "math", "statist"), corlimit = 0.5)
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 30), corlimit = 0.5)
# Sentiment Scores
syuzhet_vector <- get_sentiment(text, method = "syuzhet")
head(syuzhet_vector)
summary(syuzhet_vector)
bing_vector <- get_sentiment(text, method = "bing")
head(bing_vector)
summary(bing_vector)
afinn_vector <- get_sentiment(text, method = "afinn")
head(afinn_vector)
summary(afinn_vector)
rbind(
sign(head(syuzhet_vector)),
sign(head(bing_vector)),
sign(head(afinn_vector))
)
# Emotion Classification
d <- get_nrc_sentiment(as.vector(dtm_d$word))
head(d, 10)
td <- data.frame(t(d))
td_new <- data.frame(rowSums(td[1:56]))
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2 <- td_new[1:8,]
# Plot One - count of words associated with each sentiment
quickplot(sentiment, data = td_new2, weight = count, geom = "bar", fill = sentiment, ylab = "count") + ggtitle("Survey sentiments")
# Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
sort(colSums(prop.table(d[, 1:8]))),
horiz = TRUE,
cex.names = 0.7,
las = 1,
main = "Emotions in Text", xlab = "Percentage"
)
# Konstruowanie grafów powiązań
fileName <- 'Wikipedia.txt'
text <- readChar(fileName, file.info(fileName)$size)
text_df <- data_frame(line = 1, text = iconv(text, "cp1250", "CP852"))
text_df
tidy_text <- text_df %>%
unnest_tokens(word, text)
data(stop_words)
de <- data.frame("thy", "OLD_WORDS")
names(de) <- c("word", "lexicon")
stop_words <- rbind(stop_words, de)
de <- data.frame("1", "OLD_WORDS")
names(de) <- c("word", "lexicon")
de <- data.frame("hath", "OLD_WORDS")
names(de) <- c("word", "lexicon")
de <- data.frame("mar’d", "OLD_WORDS")
names(de) <- c("word", "lexicon")
stop_words <- rbind(stop_words, de)
tidy_text <- tidy_text %>%
anti_join(stop_words)
tidy_text %>%
count(word, sort = TRUE)
# Bigrams
text_bigrams <- text_df %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
text_bigrams %>%
count(bigram, sort = TRUE)
bigrams_separated <- text_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
# Konstruowanie grafów
bigram_graph <- bigram_counts %>%
filter(word1 == "lord" | word2 == "lord") %>%
graph_from_data_frame()
bigram_graph4 <- bigram_counts %>%
filter(word1 == "marriage" | word2 == "marriage") %>%
graph_from_data_frame()
bigram_graph5 <- bigram_counts %>%
filter(word1 == "prince" | word2 == "prince") %>%
graph_from_data_frame()
# Plotting
dev.new()
ggraph(bigram_graph4, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = TRUE, arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), position = "identity") +
theme_void()
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
library("tidytext")
library("igraph")
library("ggraph")
library(dplyr)
library(tidyr)
text <- readLines('./Wikipedia.txt')
print(text)
# Load the data as a corpus
TextDoc <- Corpus(VectorSource(text))
# Cleaning up Text Data
toSpace <- content_transformer(function (x, pattern) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
TextDoc <- tm_map(TextDoc, toSpace, "ˆa“")
TextDoc <- tm_map(TextDoc, toSpace, ":")
TextDoc <- tm_map(TextDoc, toSpace, ";")
TextDoc <- tm_map(TextDoc, toSpace, ",")
TextDoc <- tm_map(TextDoc, toSpace, "ˆI\nTM")
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
TextDoc <- tm_map(TextDoc, removeNumbers)
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team"))
TextDoc <- tm_map(TextDoc, removePunctuation)
TextDoc <- tm_map(TextDoc, stripWhitespace)
TextDoc <- tm_map(TextDoc, stemDocument)
TextDoc <- tm_map(TextDoc, content_transformer(function(x) gsub(x, pattern = "mathemat", replacement = "math")))
TextDoc <- tm_map(TextDoc, content_transformer(function(x) gsub(x, pattern = " r ", replacement = " Rlanguage ")))
# Building the term document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
dtm_v <- sort(rowSums(dtm_m), decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v), freq = dtm_v)
# Display the top 5 most frequent words
head(dtm_d, 5)
# Plot the most frequent words
barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word, col = "lightgreen",
main = "Top 20 most frequent words in the forms of knowledge\nassessment for the Big Data training courses",
ylab = "Word frequencies")
# Generate the Word Cloud
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, scale = c(5, 0.5), min.freq = 1,
max.words = 100, random.order = FALSE, rot.per = 0.40, colors = brewer.pal(8, "Dark2"))
# Word Association
findAssocs(TextDoc_dtm, terms = c("program", "algorithm", "math", "statist"), corlimit = 0.5)
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 30), corlimit = 0.5)
# Sentiment Scores
syuzhet_vector <- get_sentiment(text, method = "syuzhet")
head(syuzhet_vector)
summary(syuzhet_vector)
bing_vector <- get_sentiment(text, method = "bing")
head(bing_vector)
summary(bing_vector)
afinn_vector <- get_sentiment(text, method = "afinn")
head(afinn_vector)
summary(afinn_vector)
rbind(
sign(head(syuzhet_vector)),
sign(head(bing_vector)),
sign(head(afinn_vector))
)
# Emotion Classification
d <- get_nrc_sentiment(as.vector(dtm_d$word))
head(d, 10)
td <- data.frame(t(d))
td_new <- data.frame(rowSums(td[1:56]))
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2 <- td_new[1:8,]
# Plot One - count of words associated with each sentiment
quickplot(sentiment, data = td_new2, weight = count, geom = "bar", fill = sentiment, ylab = "count") + ggtitle("Survey sentiments")
# Plot two - count of words associated with each sentiment, expressed as a percentage
barplot(
sort(colSums(prop.table(d[, 1:8]))),
horiz = TRUE,
cex.names = 0.7,
las = 1,
main = "Emotions in Text", xlab = "Percentage"
)
# Konstruowanie grafów powiązań
fileName <- 'Wikipedia.txt'
text <- readChar(fileName, file.info(fileName)$size)
text_df <- data_frame(line = 1, text = iconv(text, "cp1250", "CP852"))
text_df
tidy_text <- text_df %>%
unnest_tokens(word, text)
data(stop_words)
de <- data.frame("thy", "OLD_WORDS")
names(de) <- c("word", "lexicon")
stop_words <- rbind(stop_words, de)
de <- data.frame("1", "OLD_WORDS")
names(de) <- c("word", "lexicon")
de <- data.frame("hath", "OLD_WORDS")
names(de) <- c("word", "lexicon")
de <- data.frame("mar’d", "OLD_WORDS")
names(de) <- c("word", "lexicon")
stop_words <- rbind(stop_words, de)
tidy_text <- tidy_text %>%
anti_join(stop_words)
tidy_text %>%
count(word, sort = TRUE)
# Bigrams
text_bigrams <- text_df %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
text_bigrams %>%
count(bigram, sort = TRUE)
bigrams_separated <- text_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
# Konstruowanie grafów
bigram_graph <- bigram_counts %>%
filter(word1 == "lord" | word2 == "lord") %>%
graph_from_data_frame()
bigram_graph4 <- bigram_counts %>%
filter(word1 == "marriage" | word2 == "marriage") %>%
graph_from_data_frame()
bigram_graph5 <- bigram_counts %>%
filter(word1 == "prince" | word2 == "prince") %>%
graph_from_data_frame()
# Plotting
dev.new()
ggraph(bigram_graph4, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = TRUE, arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), position = "identity") +
theme_void()
