To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "sequential"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d (Conv2D)                      │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d (MaxPooling2D)         │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_1 (Conv2D)                    │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_1 (MaxPooling2D)       │ (None, 5, 5, 64)            │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten (Flatten)                    │ (None, 1600)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout (Dropout)                    │ (None, 1600)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense (Dense)                        │ (None, 10)                  │          16,010 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 34,826 (136.04 KB)
 Trainable params: 34,826 (136.04 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 11s 23ms/step - accuracy: 0.6546 - loss: 0.9841 - val_accuracy: 0.8310 - val_loss: 0.4624
Epoch 2/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 10s 23ms/step - accuracy: 0.8284 - loss: 0.4739 - val_accuracy: 0.8618 - val_loss: 0.3906
Epoch 3/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 9s 21ms/step - accuracy: 0.8514 - loss: 0.4138 - val_accuracy: 0.8712 - val_loss: 0.3585
Epoch 4/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 10s 23ms/step - accuracy: 0.8599 - loss: 0.3894 - val_accuracy: 0.8757 - val_loss: 0.3400
Epoch 5/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 10s 23ms/step - accuracy: 0.8696 - loss: 0.3636 - val_accuracy: 0.8765 - val_loss: 0.3321
Epoch 6/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 12s 26ms/step - accuracy: 0.8747 - loss: 0.3433 - val_accuracy: 0.8882 - val_loss: 0.3094
Epoch 7/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 12s 29ms/step - accuracy: 0.8825 - loss: 0.3289 - val_accuracy: 0.8913 - val_loss: 0.3000
Epoch 8/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 20s 27ms/step - accuracy: 0.8865 - loss: 0.3181 - val_accuracy: 0.8925 - val_loss: 0.2930
Epoch 9/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 21s 28ms/step - accuracy: 0.8895 - loss: 0.2989 - val_accuracy: 0.8953 - val_loss: 0.2833
Epoch 10/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 20s 26ms/step - accuracy: 0.8895 - loss: 0.3035 - val_accuracy: 0.8950 - val_loss: 0.2833
Epoch 11/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 20s 26ms/step - accuracy: 0.8932 - loss: 0.2965 - val_accuracy: 0.9023 - val_loss: 0.2741
Epoch 12/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 10s 23ms/step - accuracy: 0.8974 - loss: 0.2835 - val_accuracy: 0.9002 - val_loss: 0.2703
Epoch 13/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 10s 25ms/step - accuracy: 0.8983 - loss: 0.2809 - val_accuracy: 0.8980 - val_loss: 0.2760
Epoch 14/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 20s 24ms/step - accuracy: 0.8990 - loss: 0.2766 - val_accuracy: 0.9008 - val_loss: 0.2642
Epoch 15/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 11s 26ms/step - accuracy: 0.9007 - loss: 0.2722 - val_accuracy: 0.9063 - val_loss: 0.2599
Test loss: 0.27445152401924133
Test accuracy: 0.8992000222206116
313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 3ms/step